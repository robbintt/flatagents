# Recursive Language Model (RLM) Main Machine
# Orchestrates the explore-decompose-process-synthesize workflow
# Based on arXiv:2512.24601 - "Recursive Language Models"

spec: flatmachine
spec_version: "0.7.7"

data:
  name: rlm-main

  context:
    # Input parameters
    task: "{{ input.task }}"
    context_content: "{{ input.context }}"
    context_length: "{{ input.context | length }}"
    estimated_tokens: "{{ (input.context | length) // 4 }}"
    max_chunk_size: "{{ input.max_chunk_size | default(16000) }}"
    max_exploration_rounds: "{{ input.max_exploration_rounds | default(5) }}"

    # Exploration state
    exploration_round: 0
    exploration_history: []
    exploration_findings: null
    suggested_chunks: []
    content_type: null
    structure_summary: null

    # Decomposition state
    needs_decomposition: false
    sub_tasks: []
    aggregation_strategy: null

    # Processing state
    sub_task_results: []

    # Final result
    final_answer: null
    confidence: null

  agents:
    explorer: ./explorer.yml
    decomposer: ./decomposer.yml
    processor: ./processor.yml
    synthesizer: ./synthesizer.yml

  # Peer machine for parallel chunk processing
  machines:
    chunk_processor: ./chunk_processor_machine.yml

  states:
    # Entry point
    start:
      type: initial
      # Initialize REPL with context via hook
      action: init_repl
      transitions:
        - to: explore

    # Exploration loop - examine the context structure
    explore:
      agent: explorer
      execution:
        type: retry
        backoffs: [2, 8, 16]
        jitter: 0.1
      input:
        task: "{{ context.task }}"
        context_length: "{{ context.context_length }}"
        estimated_tokens: "{{ context.estimated_tokens }}"
        exploration_history: "{{ context.exploration_history }}"
      output_to_context:
        exploration_round: "{{ context.exploration_round + 1 }}"
        code: "{{ output.code }}"
        ready_to_answer: "{{ output.ready_to_answer }}"
        findings: "{{ output.findings }}"
        suggested_chunks: "{{ output.suggested_chunks }}"
      on_error: explore_error
      transitions:
        - to: execute_exploration_code

    # Execute the exploration code in REPL
    execute_exploration_code:
      action: execute_repl
      transitions:
        - condition: "context.ready_to_answer == true"
          to: decompose
        - condition: "context.exploration_round >= context.max_exploration_rounds"
          to: decompose
        - to: explore

    # Handle exploration errors
    explore_error:
      action: log_error
      transitions:
        - condition: "context.exploration_round < context.max_exploration_rounds"
          to: explore
        - to: decompose

    # Decompose the task into sub-tasks
    decompose:
      agent: decomposer
      execution:
        type: retry
        backoffs: [2, 8]
      input:
        task: "{{ context.task }}"
        context_length: "{{ context.context_length }}"
        estimated_tokens: "{{ context.estimated_tokens }}"
        content_type: "{{ context.content_type }}"
        structure_summary: "{{ context.structure_summary }}"
        exploration_findings: "{{ context.exploration_findings }}"
        suggested_chunks: "{{ context.suggested_chunks }}"
        max_chunk_size: "{{ context.max_chunk_size }}"
      output_to_context:
        needs_decomposition: "{{ output.needs_decomposition }}"
        sub_tasks: "{{ output.sub_tasks }}"
        aggregation_strategy: "{{ output.aggregation_strategy }}"
        direct_answer: "{{ output.direct_answer }}"
      transitions:
        - condition: "context.needs_decomposition == false"
          to: direct_done
        - to: process_chunks

    # Process all chunks in parallel using foreach
    process_chunks:
      foreach: "{{ context.sub_tasks }}"
      as: sub_task
      key: "{{ sub_task.id }}"
      machine: chunk_processor
      mode: settled
      timeout: 300
      input:
        task: "{{ context.task }}"
        sub_task: "{{ sub_task }}"
        context_content: "{{ context.context_content }}"
        total_length: "{{ context.context_length }}"
      output_to_context:
        sub_task_results: "{{ output | dictsort | map(attribute='1') | list }}"
      transitions:
        - to: synthesize

    # Synthesize results into final answer
    synthesize:
      agent: synthesizer
      execution:
        type: retry
        backoffs: [2, 8]
      input:
        task: "{{ context.task }}"
        aggregation_strategy: "{{ context.aggregation_strategy }}"
        sub_task_results: "{{ context.sub_task_results }}"
      output_to_context:
        final_answer: "{{ output.final_answer }}"
        confidence: "{{ output.confidence }}"
        reasoning: "{{ output.reasoning }}"
        caveats: "{{ output.caveats }}"
      transitions:
        - to: done

    # Direct answer without decomposition
    direct_done:
      type: final
      output:
        answer: "{{ context.direct_answer }}"
        method: "direct"
        exploration_rounds: "{{ context.exploration_round }}"

    # Final state with synthesized answer
    done:
      type: final
      output:
        answer: "{{ context.final_answer }}"
        confidence: "{{ context.confidence }}"
        reasoning: "{{ context.reasoning }}"
        caveats: "{{ context.caveats }}"
        method: "recursive"
        exploration_rounds: "{{ context.exploration_round }}"
        sub_tasks_processed: "{{ context.sub_task_results | length }}"

  settings:
    max_steps: 50
    parallel_fallback: sequential

  hooks:
    file: "../src/rlm/hooks.py"
    class: "RLMHooks"

metadata:
  description: "Recursive Language Model - handles arbitrarily long contexts via REPL exploration and recursive decomposition"
  tags: ["rlm", "long-context", "recursive"]
